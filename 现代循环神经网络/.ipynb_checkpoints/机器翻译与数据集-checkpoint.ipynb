{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e513e37f",
   "metadata": {},
   "source": [
    "## 机器翻译与数据集\n",
    "语⾔模型是⾃然语⾔处理的关键，⽽机器翻译是语⾔模型最成功的基准测试。因为机器翻译正是将输⼊序列\n",
    "转换成输出序列的 序列转换模型（sequence transduction）的核⼼问题。序列转换模型在各类现代⼈⼯智能\n",
    "应⽤中发挥着⾄关重要的作⽤，因此我们将其做为本章剩余部分和 10节的重点。为此，本节将介绍机器翻译\n",
    "问题及其后⽂需要使⽤的数据集。\n",
    "\n",
    "机器翻译（machine translation）指的是将序列从⼀种语⾔⾃动翻译成另⼀种语⾔。事实上，这个研究领域可\n",
    "以追溯到数字计算机发明后不久的20世纪40年代，特别是在第⼆次世界⼤战中使⽤计算机破解语⾔编码。⼏\n",
    "⼗年来，在使⽤神经⽹络进⾏端到端学习的兴起之前，统计学⽅法在这⼀领域⼀直占据主导地位 (Brown et\n",
    "al., 1990, Brown et al., 1988)。因为统计机器翻译（statistical machine translation）涉及了翻译模型和语⾔模\n",
    "型等组成部分的统计分析，因此基于神经⽹络的⽅法通常被称为神经机器翻译（neuralmachinetranslation），\n",
    "⽤于将两种翻译模型区分开来。\n",
    "\n",
    "本书的关注点是神经⽹络机器翻译⽅法，强调的是端到端的学习。与 8.3节中的语料库是单⼀语⾔的语⾔模\n",
    "型问题存在不同，机器翻译的数据集是由源语⾔和⽬标语⾔的⽂本序列对组成的。因此，我们需要⼀种完全\n",
    "不同的⽅法来预处理机器翻译数据集，⽽不是复⽤语⾔模型的预处理程序。下⾯，我们看⼀下如何将预处理\n",
    "后的数据加载到⼩批量中⽤于训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb74a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438f1a0",
   "metadata": {},
   "source": [
    "### 下载和预处理数据集\n",
    "⾸先，下载⼀个由Tatoeba项⽬的双语句⼦对 113 组成的“英－法”数据集，数据集中的每⼀⾏都是制表符分隔\n",
    "的⽂本序列对，序列对由英⽂⽂本序列和翻译后的法语⽂本序列组成。请注意，每个⽂本序列可以是⼀个句\n",
    "⼦，也可以是包含多个句⼦的⼀个段落。在这个将英语翻译成法语的机器翻译问题中，英语是源语⾔（source\n",
    "language），法语是⽬标语⾔（target language）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d16b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @save\n",
    "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',\n",
    "                           '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
    "\n",
    "# d2l.read_data_nmt()\n",
    "def read_data_nmt():\n",
    "    \"\"\"载入“英语-法语”数据集\"\"\"\n",
    "    data_dir = d2l.download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r',\n",
    "              encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5543e65",
   "metadata": {},
   "source": [
    "下载数据集后，原始⽂本数据需要经过⼏个预处理步骤。例如，我们⽤空格代替不间断空格（non-breaking\n",
    "space），使⽤⼩写字⺟替换⼤写字⺟，并在单词和标点符号之间插⼊空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4562555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l.preprocess_nmt(text)\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"预处理“英语-法语”数据集\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "    \n",
    "    # 使用空格替换不间断空格\n",
    "    # 使用小写字母替换大写字母\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # 在单词和标点符号之间插入空格\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i-1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    \n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f82144",
   "metadata": {},
   "source": [
    "### 词元化\n",
    "与 8.3节中的字符级词元化不同，在机器翻译中，我们更喜欢单词级词元化（最先进的模型可能使⽤更⾼级\n",
    "的词元化技术）。下⾯的tokenize_nmt函数对前num_examples个⽂本序列对进⾏词元，其中每个词元要么是\n",
    "⼀个词，要么是⼀个标点符号。此函数返回两个词元列表：source和target：source[i]是源语⾔（这⾥是英\n",
    "语）第i个⽂本序列的词元列表，target[i]是⽬标语⾔（这⾥是法语）第i个⽂本序列的词元列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l.tokenize_nmt(text, num_examples=None)\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"词元化“英语-法语”数据数据集\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7966be",
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = tokenize_nmt(text)\n",
    "source[:6], target[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741573c1",
   "metadata": {},
   "source": [
    "让我们绘制每个⽂本序列所包含的词元数量的直⽅图。在这个简单的“英－法”数据集中，⼤多数⽂本序列\n",
    "的词元数量少于20个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l.show_list_len_pair_hist()\n",
    "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
    "    \"\"\"绘制列表长度对的直方图\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    _, _, patches = d2l.plt.hist(\n",
    "        [[len(l) for l in xlist], [len(l) for l in ylist]])\n",
    "    d2l.plt.xlabel(xlabel)\n",
    "    d2l.plt.ylabel(ylabel)\n",
    "    for patch in patches[1].patches:\n",
    "        patch.set_hatch('/')\n",
    "    d2l.plt.legend(legend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_list_len_pair_hist(['source', 'target'], '# tokens per sequence',\n",
    "                        'count', source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e67966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
